# Fake News Detection

This project is the **Capstone Project** for **Machine Learning & Data Mining (IT3191E)** course in **2024.2** semester at **Hanoi University of Science and Technology (HUST)**.

## üîç Objective

We experimented with and compared the performance of various pre-trained transformer models:

- **BERT** (Bidirectional Encoder Representations from Transformers)
- **RoBERTa** (Robustly Optimized BERT Pretraining Approach)
- **DeBERTa** (Decoding-enhanced BERT with disentangled attention)
- **XLNet** (Generalized Autoregressive Pretraining for Language Understanding)
- **XLM-RoBERTa** (Cross-lingual Language Model based on RoBERTa)
- **ELECTRA** (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)

All models were fine-tuned on labeled datasets for fake news detection, and evaluated using common classification metrics.

## üèóÔ∏è Technologies & Frameworks

- Python 3.8+
- PyTorch / Transformers (by HuggingFace)
- Scikit-learn
- Pandas / NumPy
- Jupyter Notebook

## üë• Group Information

**Instructor:** Ph.D. Nguyen Duc Anh  
**Group:** 1  

| No. | Name               | Student ID | Role         |
|-----|--------------------|------------|--------------|
| 1   | Ho Bao Thu         | 20226003   | Team Leader  |
| 2   | Tran Kim Cuong     | 20226017   | Member       |
| 3   | Nguyen Dinh Duong  | 20225966   | Member       |
| 4   | Nguyen My Duyen    | 20225967   | Member       |
| 5   | Ha Viet Khanh      | 20225979   | Member       |
| 6   | Dang Van Nhan      | 20225990   | Member       |


